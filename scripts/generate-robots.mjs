#!/usr/bin/env node

/**
 * Generates public/robots.txt at build time for OpenNext/SST deployment.
 *
 * WHY THIS EXISTS:
 * OpenNext checks S3 for static files BEFORE route handlers. If robots.txt
 * was ever in public/, OpenNext expects it in S3. The route handler
 * (app/robots.txt/route.ts) is NEVER invoked because S3 takes precedence.
 *
 * This script generates public/robots.txt at build time, ensuring the
 * correct content is deployed to S3 automatically.
 *
 * See: docs/incidents/2025-12-23-robots-txt-routing-issues.md
 */

import { writeFileSync } from "fs";
import { join, dirname } from "path";
import { fileURLToPath } from "url";

const __dirname = dirname(fileURLToPath(import.meta.url));
const rootDir = join(__dirname, "..");

// Get site URL from environment or use default
const siteUrl =
  process.env.NEXT_PUBLIC_SITE_URL ||
  process.env.NEXT_PUBLIC_VERCEL_URL ||
  "https://www.esdeveniments.cat";

// Normalize URL (remove trailing slash, ensure https)
const normalizedSiteUrl = siteUrl.replace(/\/$/, "").replace(/^http:/, "https:");

/**
 * robots.txt configuration - KEEP IN SYNC with app/robots.txt/route.ts
 *
 * 2025 SEO Best Practices:
 * - Allow search engine crawlers (Googlebot, Bingbot, etc.)
 * - Block AI training crawlers (GPTBot, CCBot, etc.) to protect content
 * - Block /_next/ static files (JS chunks, CSS, build artifacts)
 * - Block /api/ routes (internal endpoints, not for indexing)
 */
const robotsConfig = {
  rules: [
    // Default rules for all crawlers (including search engines)
    {
      userAgent: "*",
      allow: ["/"],
      disallow: ["/_next/", "/api/", "/e2e/", "/offline/", "/login/"],
    },
    // Block AI TRAINING crawlers (not search/browsing crawlers)
    { userAgent: "GPTBot", disallow: ["/"] },
    { userAgent: "CCBot", disallow: ["/"] },
    { userAgent: "Google-Extended", disallow: ["/"] },
    { userAgent: "Bytespider", disallow: ["/"] },
    { userAgent: "anthropic-ai", disallow: ["/"] },
    { userAgent: "ClaudeBot", disallow: ["/"] },
    { userAgent: "Applebot-Extended", disallow: ["/"] },
    { userAgent: "Meta-ExternalAgent", disallow: ["/"] },
    { userAgent: "cohere-ai", disallow: ["/"] },
    { userAgent: "Omgilibot", disallow: ["/"] },
  ],
  sitemaps: [
    `${normalizedSiteUrl}/sitemap.xml`,
    `${normalizedSiteUrl}/server-static-sitemap.xml`,
    `${normalizedSiteUrl}/server-sitemap.xml`,
    `${normalizedSiteUrl}/server-news-sitemap.xml`,
    `${normalizedSiteUrl}/server-place-sitemap.xml`,
    `${normalizedSiteUrl}/server-google-news-sitemap.xml`,
  ],
  host: normalizedSiteUrl,
};

function generateRobotsTxt() {
  const lines = [];

  // Add generation timestamp
  lines.push(`# Generated by scripts/generate-robots.mjs at ${new Date().toISOString()}`);
  lines.push(`# Site URL: ${normalizedSiteUrl}`);
  lines.push("");

  // Add rules
  for (const rule of robotsConfig.rules) {
    lines.push(`User-Agent: ${rule.userAgent}`);

    if (rule.allow) {
      for (const path of rule.allow) {
        lines.push(`Allow: ${path}`);
      }
    }

    if (rule.disallow) {
      for (const path of rule.disallow) {
        lines.push(`Disallow: ${path}`);
      }
    }

    lines.push("");
  }

  // Add host
  lines.push(`Host: ${robotsConfig.host}`);
  lines.push("");

  // Add sitemaps
  for (const sitemap of robotsConfig.sitemaps) {
    lines.push(`Sitemap: ${sitemap}`);
  }

  return lines.join("\n");
}

// Generate and write robots.txt
const robotsTxt = generateRobotsTxt();
const outputPath = join(rootDir, "public", "robots.txt");

writeFileSync(outputPath, robotsTxt, "utf-8");

console.log(`âœ… Generated ${outputPath}`);
console.log(`   Site URL: ${normalizedSiteUrl}`);
console.log(`   AI training bots blocked: ${robotsConfig.rules.length - 1}`);
